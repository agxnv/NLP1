{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McArD4rSDR2K"
      },
      "source": [
        "### Consigna del desafío 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJgf6GQIIEH1"
      },
      "source": [
        "**1**. Vectorizar documentos. Tomar 5 documentos al azar y medir similaridad con el resto de los documentos.\n",
        "Estudiar los 5 documentos más similares de cada uno analizar si tiene sentido\n",
        "la similaridad según el contenido del texto y la etiqueta de clasificación."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import random\n",
        "\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Cargar datos\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "tfidfvect = TfidfVectorizer()\n",
        "X_train = tfidfvect.fit_transform(newsgroups_train.data)\n",
        "y_train = newsgroups_train.target\n",
        "\n",
        "\n",
        "random_docs = random.sample(range(len(newsgroups_train.data)), 5)\n",
        "\n",
        "for i, doc_idx in enumerate(random_docs):\n",
        "    print(f\"--- DOCUMENTO {i+1} (índice {doc_idx}) ---\")\n",
        "    print(f\"Clase: {newsgroups_train.target_names[y_train[doc_idx]]}\")\n",
        "    print(f\"Texto (primeros 200 chars): {newsgroups_train.data[doc_idx][:200]}...\")\n",
        "\n",
        "    cossim = cosine_similarity(X_train[doc_idx], X_train)[0]\n",
        "    most_similar = np.argsort(cossim)[::-1][1:6]\n",
        "\n",
        "    print(\"5 documentos más similares:\")\n",
        "    for j, sim_idx in enumerate(most_similar):\n",
        "        sim_class = newsgroups_train.target_names[y_train[sim_idx]]\n",
        "        print(f\"  {j+1}. Similaridad: {cossim[sim_idx]:.4f}, Clase: {sim_class}\")\n",
        "    print()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f17u_EdVyvwb",
        "outputId": "190b8590-039f-4f51-b497-3c8487c447d9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- DOCUMENTO 1 (índice 10476) ---\n",
            "Clase: rec.sport.hockey\n",
            "Texto (primeros 200 chars): This is a general question for US readers:\n",
            "\n",
            "How extensive is the playoff coverage down there?  In Canada, it is almost\n",
            "impossible not to watch a series on TV (ie the only two series I have not had\n",
            "an ...\n",
            "5 documentos más similares:\n",
            "  1. Similaridad: 0.2250, Clase: rec.sport.hockey\n",
            "  2. Similaridad: 0.2174, Clase: talk.politics.mideast\n",
            "  3. Similaridad: 0.2164, Clase: sci.crypt\n",
            "  4. Similaridad: 0.2126, Clase: alt.atheism\n",
            "  5. Similaridad: 0.2111, Clase: sci.crypt\n",
            "\n",
            "--- DOCUMENTO 2 (índice 1824) ---\n",
            "Clase: comp.sys.mac.hardware\n",
            "Texto (primeros 200 chars): \n",
            "\n",
            "\tI think this kind of comparison is pretty useless in general.  The\n",
            "processor is only good when a good computer is designed around it adn the\n",
            "computer is used in its designed purpose.  Comparing pro...\n",
            "5 documentos más similares:\n",
            "  1. Similaridad: 0.3542, Clase: comp.sys.mac.hardware\n",
            "  2. Similaridad: 0.3132, Clase: comp.sys.mac.hardware\n",
            "  3. Similaridad: 0.3041, Clase: comp.sys.mac.hardware\n",
            "  4. Similaridad: 0.2504, Clase: comp.sys.mac.hardware\n",
            "  5. Similaridad: 0.2417, Clase: comp.sys.mac.hardware\n",
            "\n",
            "--- DOCUMENTO 3 (índice 409) ---\n",
            "Clase: comp.graphics\n",
            "Texto (primeros 200 chars): I can't fiqure this out.  I have properly compiled pov on a unix machine\n",
            "running SunOS 4.1.3  The problem is that when I run the sample .pov files and\n",
            "use the EXACT same parameters when compiling diff...\n",
            "5 documentos más similares:\n",
            "  1. Similaridad: 0.2305, Clase: comp.graphics\n",
            "  2. Similaridad: 0.2091, Clase: comp.graphics\n",
            "  3. Similaridad: 0.1982, Clase: comp.graphics\n",
            "  4. Similaridad: 0.1838, Clase: comp.graphics\n",
            "  5. Similaridad: 0.1659, Clase: comp.graphics\n",
            "\n",
            "--- DOCUMENTO 4 (índice 4506) ---\n",
            "Clase: rec.autos\n",
            "Texto (primeros 200 chars): \n",
            "This does sound good, but I heard it tends to leave more grit, etc in the \n",
            "oil pan.  Also, I've been told to change the old when it's hot before the\n",
            "grit has much time to settle.\n",
            "\n",
            "Any opinions?\n",
            "...\n",
            "5 documentos más similares:\n",
            "  1. Similaridad: 0.1894, Clase: rec.motorcycles\n",
            "  2. Similaridad: 0.1682, Clase: comp.sys.mac.hardware\n",
            "  3. Similaridad: 0.1583, Clase: rec.autos\n",
            "  4. Similaridad: 0.1577, Clase: rec.autos\n",
            "  5. Similaridad: 0.1522, Clase: rec.autos\n",
            "\n",
            "--- DOCUMENTO 5 (índice 4012) ---\n",
            "Clase: rec.sport.hockey\n",
            "Texto (primeros 200 chars): For those Leaf fans who are concerned, the following players are slated for\n",
            "return on Thursday's Winnipeg-Toronto game :\n",
            "    Peter Zezel, John Cullen\n",
            "\n",
            "  Mark Osborne and Dave Ellett are questionable t...\n",
            "5 documentos más similares:\n",
            "  1. Similaridad: 0.1600, Clase: soc.religion.christian\n",
            "  2. Similaridad: 0.1428, Clase: rec.sport.hockey\n",
            "  3. Similaridad: 0.1358, Clase: rec.sport.hockey\n",
            "  4. Similaridad: 0.1318, Clase: rec.sport.hockey\n",
            "  5. Similaridad: 0.1307, Clase: rec.sport.baseball\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2**. Construir un modelo de clasificación por prototipos (tipo zero-shot). Clasificar los documentos de un conjunto de test comparando cada uno con todos los de entrenamiento y asignar la clase al label del documento del conjunto de entrenamiento con mayor similaridad."
      ],
      "metadata": {
        "id": "iBWIMNEE0PQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = tfidfvect.transform(newsgroups_test.data)\n",
        "y_test = newsgroups_test.target\n",
        "\n",
        "def prototype_classifier(X_train, y_train, X_test):\n",
        "    predictions = []\n",
        "\n",
        "    for i in range(X_test.shape[0]):\n",
        "        similarities = cosine_similarity(X_test[i], X_train)[0]\n",
        "        most_similar_idx = np.argmax(similarities)\n",
        "        predictions.append(y_train[most_similar_idx])\n",
        "\n",
        "    return np.array(predictions)\n",
        "\n",
        "# Clasificar usando prototipos\n",
        "y_pred_prototype = prototype_classifier(X_train, y_train, X_test)\n",
        "f1_prototype = f1_score(y_test, y_pred_prototype, average='macro')\n",
        "print(f\"F1-score: {f1_prototype:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2TS0UWNy2eg",
        "outputId": "4583a8b9-fa1f-459b-f2b3-e0c1d4164d25"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-score: 0.5050\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3**. Entrenar modelos de clasificación Naïve Bayes para maximizar el desempeño de clasificación\n",
        "(f1-score macro) en el conjunto de datos de test. Considerar cambiar parámteros\n",
        "de instanciación del vectorizador y los modelos y probar modelos de Naïve Bayes Multinomial\n",
        "y ComplementNB."
      ],
      "metadata": {
        "id": "fBgSIMIw0R-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "best_f1 = 0\n",
        "best_config = {}\n",
        "\n",
        "vectorizer_configs = [\n",
        "    {'max_features': None, 'ngram_range': (1, 1), 'min_df': 1, 'max_df': 1.0},\n",
        "    {'max_features': 10000, 'ngram_range': (1, 1), 'min_df': 2, 'max_df': 0.95},\n",
        "    {'max_features': 20000, 'ngram_range': (1, 2), 'min_df': 2, 'max_df': 0.95},\n",
        "    {'max_features': 15000, 'ngram_range': (1, 2), 'min_df': 3, 'max_df': 0.90},\n",
        "    {'max_features': None, 'ngram_range': (1, 1), 'min_df': 5, 'max_df': 0.85}\n",
        "]\n",
        "\n",
        "models = [MultinomialNB(), ComplementNB()]\n",
        "model_names = ['MultinomialNB', 'ComplementNB']\n",
        "\n",
        "for vec_config in vectorizer_configs:\n",
        "    vectorizer = TfidfVectorizer(**vec_config)\n",
        "    X_train_vec = vectorizer.fit_transform(newsgroups_train.data)\n",
        "    X_test_vec = vectorizer.transform(newsgroups_test.data)\n",
        "\n",
        "    for model, model_name in zip(models, model_names):\n",
        "        model.fit(X_train_vec, y_train)\n",
        "        y_pred = model.predict(X_test_vec)\n",
        "        f1 = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "        print(f\"{model_name} con {vec_config}: F1 = {f1:.4f}\")\n",
        "\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            best_config = {'vectorizer': vec_config, 'model': model_name, 'f1': f1}\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGHXYigEy76H",
        "outputId": "415af4d8-dbc6-4a3f-dcf0-106aaade9493"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MultinomialNB con {'max_features': None, 'ngram_range': (1, 1), 'min_df': 1, 'max_df': 1.0}: F1 = 0.5854\n",
            "ComplementNB con {'max_features': None, 'ngram_range': (1, 1), 'min_df': 1, 'max_df': 1.0}: F1 = 0.6930\n",
            "MultinomialNB con {'max_features': 10000, 'ngram_range': (1, 1), 'min_df': 2, 'max_df': 0.95}: F1 = 0.6149\n",
            "ComplementNB con {'max_features': 10000, 'ngram_range': (1, 1), 'min_df': 2, 'max_df': 0.95}: F1 = 0.6665\n",
            "MultinomialNB con {'max_features': 20000, 'ngram_range': (1, 2), 'min_df': 2, 'max_df': 0.95}: F1 = 0.5859\n",
            "ComplementNB con {'max_features': 20000, 'ngram_range': (1, 2), 'min_df': 2, 'max_df': 0.95}: F1 = 0.6639\n",
            "MultinomialNB con {'max_features': 15000, 'ngram_range': (1, 2), 'min_df': 3, 'max_df': 0.9}: F1 = 0.5848\n",
            "ComplementNB con {'max_features': 15000, 'ngram_range': (1, 2), 'min_df': 3, 'max_df': 0.9}: F1 = 0.6517\n",
            "MultinomialNB con {'max_features': None, 'ngram_range': (1, 1), 'min_df': 5, 'max_df': 0.85}: F1 = 0.6098\n",
            "ComplementNB con {'max_features': None, 'ngram_range': (1, 1), 'min_df': 5, 'max_df': 0.85}: F1 = 0.6838\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mejor modelo**: ComplementNB, **Vectorizador**: {'max_features': None, 'ngram_range': (1, 1), 'min_df': 1, 'max_df': 1.0}. **F1-score**: 0.6930"
      ],
      "metadata": {
        "id": "UExDO4jScbyY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4**. Transponer la matriz documento-término. De esa manera se obtiene una matriz\n",
        "término-documento que puede ser interpretada como una colección de vectorización de palabras.\n",
        "Estudiar ahora similaridad entre palabras tomando 5 palabras y estudiando sus 5 más similares. **La elección de palabras no debe ser al azar para evitar la aparición de términos poco interpretables, elegirlas \"manualmente\"**."
      ],
      "metadata": {
        "id": "7CYJBfZ80U_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx2word = {v: k for k, v in tfidfvect.vocabulary_.items()}\n",
        "X_term_doc = X_train.T\n",
        "\n",
        "target_words = ['computer', 'car', 'science', 'religion', 'medical']\n",
        "\n",
        "print(\"Similaridad entre palabras:\")\n",
        "for word in target_words:\n",
        "    if word in tfidfvect.vocabulary_:\n",
        "        word_idx = tfidfvect.vocabulary_[word]\n",
        "\n",
        "        word_similarities = cosine_similarity(X_term_doc[word_idx], X_term_doc)[0]\n",
        "\n",
        "        most_similar_indices = np.argsort(word_similarities)[::-1][1:6]\n",
        "\n",
        "        print(f\"\\nPalabra: '{word}'\")\n",
        "        print(\"5 palabras más similares:\")\n",
        "        for i, sim_idx in enumerate(most_similar_indices):\n",
        "            similar_word = idx2word[sim_idx]\n",
        "            similarity = word_similarities[sim_idx]\n",
        "            print(f\"  {i+1}. '{similar_word}' (similaridad: {similarity:.4f})\")\n",
        "\n",
        "print(f\"F1-score clasificador por prototipos: {f1_prototype:.4f}\")\n",
        "print(f\"Mejor F1-score Naive Bayes optimizado: {best_f1:.4f}\")\n",
        "print(f\"Mejora: {((best_f1 - f1_prototype) / f1_prototype * 100):.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4Tm8pGfy97v",
        "outputId": "1fc7476a-dd46-45fc-94fc-cb21eda27dae"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similaridad entre palabras:\n",
            "\n",
            "Palabra: 'computer'\n",
            "5 palabras más similares:\n",
            "  1. 'decwriter' (similaridad: 0.1563)\n",
            "  2. 'deluged' (similaridad: 0.1522)\n",
            "  3. 'harkens' (similaridad: 0.1522)\n",
            "  4. 'shopper' (similaridad: 0.1443)\n",
            "  5. 'the' (similaridad: 0.1361)\n",
            "\n",
            "Palabra: 'car'\n",
            "5 palabras más similares:\n",
            "  1. 'cars' (similaridad: 0.1797)\n",
            "  2. 'criterium' (similaridad: 0.1770)\n",
            "  3. 'civic' (similaridad: 0.1748)\n",
            "  4. 'owner' (similaridad: 0.1689)\n",
            "  5. 'dealer' (similaridad: 0.1681)\n",
            "\n",
            "Palabra: 'science'\n",
            "5 palabras más similares:\n",
            "  1. 'cognitivists' (similaridad: 0.3948)\n",
            "  2. 'behaviorists' (similaridad: 0.3948)\n",
            "  3. 'scientific' (similaridad: 0.3466)\n",
            "  4. 'empirical' (similaridad: 0.2810)\n",
            "  5. 'sects' (similaridad: 0.2622)\n",
            "\n",
            "Palabra: 'religion'\n",
            "5 palabras más similares:\n",
            "  1. 'religious' (similaridad: 0.2451)\n",
            "  2. 'religions' (similaridad: 0.2116)\n",
            "  3. 'categorized' (similaridad: 0.2039)\n",
            "  4. 'purpsoe' (similaridad: 0.2008)\n",
            "  5. 'crusades' (similaridad: 0.1987)\n",
            "\n",
            "Palabra: 'medical'\n",
            "5 palabras más similares:\n",
            "  1. 'romano' (similaridad: 0.2823)\n",
            "  2. 'hospitals' (similaridad: 0.2751)\n",
            "  3. 'recuperation' (similaridad: 0.2682)\n",
            "  4. 'providers' (similaridad: 0.2391)\n",
            "  5. 'relelvant' (similaridad: 0.2278)\n",
            "F1-score clasificador por prototipos: 0.5050\n",
            "Mejor F1-score Naive Bayes optimizado: 0.6930\n",
            "Mejora: 37.22%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}